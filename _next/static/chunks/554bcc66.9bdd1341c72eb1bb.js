"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[373],{1401:(e,t,s)=>{s.d(t,{WF:()=>k,v6:()=>tt});var n=s(6602),i=s(46),r=s(3540),o=s(3164),l=s(6085),a=s(6702),c=s(5825),h=s(8909);async function u(e,t){let s=await Promise.all([(0,r.wc)(e,"tokenizer.json",!0,t),(0,r.wc)(e,"tokenizer_config.json",!0,t)]);return null!==t.legacy&&(s[1].legacy=t.legacy),s}function d(e,t=!0){if(void 0!==e.Regex){let t=e.Regex.replace(/\\([#&~])/g,"$1");for(let[e,s]of b)t=t.replaceAll(e,s);return RegExp(t,"gu")}if(void 0===e.String)return console.warn("Unknown pattern type:",e),null;{let s=(0,i.Nt)(e.String);return RegExp(t?s:`(${s})`,"gu")}}function _(e){return new Map(Object.entries(e))}function p(e){let t=e.dims;switch(t.length){case 1:return e.tolist();case 2:if(1!==t[0])throw Error("Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.");return e.tolist()[0];default:throw Error(`Expected tensor to have 1-2 dimensions, got ${t.length}.`)}}function f(e){return e.replace(/ \./g,".").replace(/ \?/g,"?").replace(/ \!/g,"!").replace(/ ,/g,",").replace(/ \' /g,"'").replace(/ n\'t/g,"n't").replace(/ \'m/g,"'m").replace(/ \'s/g,"'s").replace(/ \'ve/g,"'ve").replace(/ \'re/g,"'re")}function g(e){return e.replace(/\p{M}/gu,"")}function k(e){return e>=19968&&e<=40959||e>=13312&&e<=19903||e>=131072&&e<=173791||e>=173824&&e<=177983||e>=177984&&e<=178207||e>=178208&&e<=183983||e>=63744&&e<=64255||e>=194560&&e<=195103}let m="\\p{P}\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E",x=RegExp(`^[${m}]+$`,"gu"),w=".,!?…。，、।۔،",b=new Map([["(?i:'s|'t|'re|'ve|'m|'ll|'d)","(?:'([sS]|[tT]|[rR][eE]|[vV][eE]|[mM]|[lL][lL]|[dD]))"],[` ?[^(\\s|[${w}])]+`,` ?[^\\s${w}]+`]]);class y{constructor(e){this.content=e.content,this.id=e.id,this.single_word=e.single_word??!1,this.lstrip=e.lstrip??!1,this.rstrip=e.rstrip??!1,this.special=e.special??!1,this.normalized=e.normalized??null}}class v extends n.b{constructor(e){super(),this.config=e,this.vocab=[],this.tokens_to_ids=new Map,this.unk_token_id=void 0,this.unk_token=void 0,this.end_of_word_suffix=void 0,this.fuse_unk=this.config.fuse_unk??!1}static fromConfig(e,...t){switch(e.type){case"WordPiece":return new z(e);case"Unigram":return new A(e,...t);case"BPE":return new S(e);default:if(e.vocab){if(Array.isArray(e.vocab))return new A(e,...t);if("object"==typeof e.vocab&&e.continuing_subword_prefix&&e.unk_token)return new z(e);return new R(e,...t)}throw Error(`Unknown TokenizerModel type: ${e.type}`)}}_call(e){return e=this.encode(e),this.fuse_unk&&(e=function(e,t,s){let n=[],i=0;for(;i<e.length;){if(n.push(e[i]),(t.get(e[i])??s)!==s){++i;continue}for(;++i<e.length&&(t.get(e[i])??s)===s;)t.get(n.at(-1))!==s&&(n[n.length-1]+=e[i])}return n}(e,this.tokens_to_ids,this.unk_token_id)),e}encode(e){throw Error("encode should be implemented in subclass.")}convert_tokens_to_ids(e){return e.map(e=>this.tokens_to_ids.get(e)??this.unk_token_id)}convert_ids_to_tokens(e){return e.map(e=>this.vocab[e]??this.unk_token)}}class z extends v{constructor(e){for(let[t,s]of(super(e),this.tokens_to_ids=_(e.vocab),this.unk_token_id=this.tokens_to_ids.get(e.unk_token),this.unk_token=e.unk_token,this.max_input_chars_per_word=e.max_input_chars_per_word??100,this.vocab=Array(this.tokens_to_ids.size),this.tokens_to_ids))this.vocab[s]=t}encode(e){let t=[];for(let s of e){let e=[...s];if(e.length>this.max_input_chars_per_word){t.push(this.unk_token);continue}let n=!1,i=0,r=[];for(;i<e.length;){let t=e.length,s=null;for(;i<t;){let n=e.slice(i,t).join("");if(i>0&&(n=this.config.continuing_subword_prefix+n),this.tokens_to_ids.has(n)){s=n;break}--t}if(null===s){n=!0;break}r.push(s),i=t}n?t.push(this.unk_token):t.push(...r)}return t}}class A extends v{constructor(e,t){super(e);let s=e.vocab.length;this.vocab=Array(s),this.scores=Array(s);for(let t=0;t<s;++t)[this.vocab[t],this.scores[t]]=e.vocab[t];this.unk_token_id=e.unk_id,this.unk_token=this.vocab[e.unk_id],this.tokens_to_ids=new Map(this.vocab.map((e,t)=>[e,t])),this.bos_token=" ",this.bos_token_id=this.tokens_to_ids.get(this.bos_token),this.eos_token=t.eos_token,this.eos_token_id=this.tokens_to_ids.get(this.eos_token),this.unk_token=this.vocab[this.unk_token_id],this.minScore=(0,o.jk)(this.scores)[0],this.unk_score=this.minScore-10,this.scores[this.unk_token_id]=this.unk_score,this.trie=new a.N_,this.trie.extend(this.vocab),this.fuse_unk=!0}populateNodes(e){let t=e.chars,s=0;for(;s<t.length;){let n=!1,r=[],o=t.slice(s).join("");for(let t of this.trie.commonPrefixSearch(o)){r.push(t);let o=this.tokens_to_ids.get(t),l=this.scores[o],a=(0,i.Il)(t);e.insert(s,a,l,o),n||1!==a||(n=!0)}n||e.insert(s,1,this.unk_score,this.unk_token_id),s+=1}}tokenize(e){let t=new a.Ah(e,this.bos_token_id,this.eos_token_id);return this.populateNodes(t),t.tokens()}encode(e){let t=[];for(let s of e){let e=this.tokenize(s);t.push(...e)}return t}}let T=(()=>{let e=[...Array.from({length:94},(e,t)=>t+33),...Array.from({length:12},(e,t)=>t+161),...Array.from({length:82},(e,t)=>t+174)],t=e.slice(),s=0;for(let n=0;n<256;++n)e.includes(n)||(e.push(n),t.push(256+s),s+=1);let n=t.map(e=>String.fromCharCode(e));return Object.fromEntries(e.map((e,t)=>[e,n[t]]))})(),E=(0,i.vp)(T);class S extends v{constructor(e){for(let[t,s]of(super(e),this.tokens_to_ids=_(e.vocab),this.unk_token_id=this.tokens_to_ids.get(e.unk_token),this.unk_token=e.unk_token,this.vocab=Array(this.tokens_to_ids.size),this.tokens_to_ids))this.vocab[s]=t;let t=Array.isArray(e.merges[0]);this.merges=t?e.merges:e.merges.map(e=>e.split(" ",2)),this.bpe_ranks=new Map(this.merges.map((e,t)=>[JSON.stringify(e),t])),this.end_of_word_suffix=e.end_of_word_suffix,this.continuing_subword_suffix=e.continuing_subword_suffix??null,this.byte_fallback=this.config.byte_fallback??!1,this.byte_fallback&&(this.text_encoder=new TextEncoder),this.ignore_merges=this.config.ignore_merges??!1,this.cache=new Map}bpe(e){if(0===e.length)return[];let t=this.cache.get(e);if(void 0!==t)return t;let s=Array.from(e);this.end_of_word_suffix&&(s[s.length-1]+=this.end_of_word_suffix);let n=[];if(s.length>1){let e=new a.Mr((e,t)=>e.score<t.score),t={token:s[0],bias:0,prev:null,next:null},i=t;for(let t=1;t<s.length;++t){let n={bias:t/s.length,token:s[t],prev:i,next:null};i.next=n,this._add_node(e,i),i=n}for(;!e.isEmpty();){let s=e.pop();if(s.deleted||!s.next||s.next.deleted)continue;if(s.deleted=!0,s.next.deleted=!0,s.prev){let e={...s.prev};s.prev.deleted=!0,s.prev=e,e.prev?e.prev.next=e:t=e}let n={token:s.token+s.next.token,bias:s.bias,prev:s.prev,next:s.next.next};n.prev?(n.prev.next=n,this._add_node(e,n.prev)):t=n,n.next&&(n.next.prev=n,this._add_node(e,n))}for(let e=t;null!==e;e=e.next)n.push(e.token)}else n=s;if(this.continuing_subword_suffix)for(let e=0;e<n.length-1;++e)n[e]+=this.continuing_subword_suffix;return this.cache.set(e,n),n}_add_node(e,t){let s=this.bpe_ranks.get(JSON.stringify([t.token,t.next.token]));void 0!==s&&(t.score=s+t.bias,e.push(t))}encode(e){let t=[];for(let s of e){if(this.ignore_merges&&this.tokens_to_ids.has(s)){t.push(s);continue}for(let e of this.bpe(s))if(this.tokens_to_ids.has(e))t.push(e);else if(this.byte_fallback){let s=Array.from(this.text_encoder.encode(e)).map(e=>`<0x${e.toString(16).toUpperCase().padStart(2,"0")}>`);s.every(e=>this.tokens_to_ids.has(e))?t.push(...s):t.push(this.unk_token)}else t.push(this.unk_token)}return t}}class R extends v{constructor(e,t){for(let[s,n]of(super(e),this.tokens_to_ids=_(t.target_lang?e.vocab[t.target_lang]:e.vocab),this.bos_token=t.bos_token,this.bos_token_id=this.tokens_to_ids.get(this.bos_token),this.eos_token=t.eos_token,this.eos_token_id=this.tokens_to_ids.get(this.eos_token),this.pad_token=t.pad_token,this.pad_token_id=this.tokens_to_ids.get(this.pad_token),this.unk_token=t.unk_token,this.unk_token_id=this.tokens_to_ids.get(this.unk_token),this.vocab=Array(this.tokens_to_ids.size),this.tokens_to_ids))this.vocab[n]=s}encode(e){return e}}class C extends n.b{constructor(e){super(),this.config=e}static fromConfig(e){if(null===e)return null;switch(e.type){case"BertNormalizer":return new U(e);case"Precompiled":return new ed(e);case"Sequence":return new L(e);case"Replace":return new $(e);case"NFC":return new j(e);case"NFKC":return new N(e);case"NFKD":return new F(e);case"Strip":return new M(e);case"StripAccents":return new P(e);case"Lowercase":return new O(e);case"Prepend":return new W(e);default:throw Error(`Unknown Normalizer type: ${e.type}`)}}normalize(e){throw Error("normalize should be implemented in subclass.")}_call(e){return this.normalize(e)}}class $ extends C{normalize(e){let t=d(this.config.pattern);return null===t?e:e.replaceAll(t,this.config.content)}}class j extends C{normalize(e){return e=e.normalize("NFC")}}class N extends C{normalize(e){return e=e.normalize("NFKC")}}class F extends C{normalize(e){return e=e.normalize("NFKD")}}class M extends C{normalize(e){return this.config.strip_left&&this.config.strip_right?e=e.trim():(this.config.strip_left&&(e=e.trimStart()),this.config.strip_right&&(e=e.trimEnd())),e}}class P extends C{normalize(e){return e=g(e)}}class O extends C{normalize(e){return e=e.toLowerCase()}}class W extends C{normalize(e){return e=this.config.prepend+e}}class L extends C{constructor(e){super(e),this.normalizers=e.normalizers.map(e=>C.fromConfig(e))}normalize(e){return this.normalizers.reduce((e,t)=>t.normalize(e),e)}}class U extends C{_tokenize_chinese_chars(e){let t=[];for(let s=0;s<e.length;++s){let n=e[s];k(n.charCodeAt(0))?(t.push(" "),t.push(n),t.push(" ")):t.push(n)}return t.join("")}stripAccents(e){return e.normalize("NFD").replace(/\p{Mn}/gu,"")}_is_control(e){switch(e){case"	":case"\n":case"\r":return!1;default:return/^\p{Cc}|\p{Cf}|\p{Co}|\p{Cs}$/u.test(e)}}_clean_text(e){let t=[];for(let s of e){let e=s.charCodeAt(0);0===e||65533===e||this._is_control(s)||(/^\s$/.test(s)?t.push(" "):t.push(s))}return t.join("")}normalize(e){return this.config.clean_text&&(e=this._clean_text(e)),this.config.handle_chinese_chars&&(e=this._tokenize_chinese_chars(e)),this.config.lowercase?(e=e.toLowerCase(),!1!==this.config.strip_accents&&(e=this.stripAccents(e))):this.config.strip_accents&&(e=this.stripAccents(e)),e}}class q extends n.b{static fromConfig(e){if(null===e)return null;switch(e.type){case"BertPreTokenizer":return new B(e);case"Sequence":return new e_(e);case"Whitespace":return new ep(e);case"WhitespaceSplit":return new ef(e);case"Metaspace":return new eh(e);case"ByteLevel":return new I(e);case"Split":return new D(e);case"Punctuation":return new K(e);case"Digits":return new G(e);case"Replace":return new eg(e);default:throw Error(`Unknown PreTokenizer type: ${e.type}`)}}pre_tokenize_text(e,t){throw Error("pre_tokenize_text should be implemented in subclass.")}pre_tokenize(e,t){return(Array.isArray(e)?e.map(e=>this.pre_tokenize_text(e,t)):this.pre_tokenize_text(e,t)).flat()}_call(e,t){return this.pre_tokenize(e,t)}}class B extends q{constructor(e){super(),this.pattern=RegExp(`[^\\s${m}]+|[${m}]`,"gu")}pre_tokenize_text(e,t){return e.trim().match(this.pattern)||[]}}class I extends q{constructor(e){super(),this.config=e,this.add_prefix_space=this.config.add_prefix_space,this.trim_offsets=this.config.trim_offsets,this.use_regex=this.config.use_regex??!0,this.pattern=/'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+/gu,this.byte_encoder=T,this.text_encoder=new TextEncoder}pre_tokenize_text(e,t){return this.add_prefix_space&&!e.startsWith(" ")&&(e=" "+e),(this.use_regex?e.match(this.pattern)||[]:[e]).map(e=>Array.from(this.text_encoder.encode(e),e=>this.byte_encoder[e]).join(""))}}class D extends q{constructor(e){super(),this.config=e,this.pattern=d(this.config.pattern,this.config.invert)}pre_tokenize_text(e,t){return null===this.pattern?[]:this.config.invert?e.match(this.pattern)||[]:this.config.behavior?.toLowerCase()==="removed"?e.split(this.pattern).filter(e=>e):function(e,t){let s=[],n=0;for(let i of e.matchAll(t)){let t=i[0];n<i.index&&s.push(e.slice(n,i.index)),t.length>0&&s.push(t),n=i.index+t.length}return n<e.length&&s.push(e.slice(n)),s}(e,this.pattern)}}class K extends q{constructor(e){super(),this.config=e,this.pattern=RegExp(`[^${m}]+|[${m}]+`,"gu")}pre_tokenize_text(e,t){return e.match(this.pattern)||[]}}class G extends q{constructor(e){super(),this.config=e;let t=`[^\\d]+|\\d${this.config.individual_digits?"":"+"}`;this.pattern=RegExp(t,"gu")}pre_tokenize_text(e,t){return e.match(this.pattern)||[]}}class Y extends n.b{constructor(e){super(),this.config=e}static fromConfig(e){if(null===e)return null;switch(e.type){case"TemplateProcessing":return new H(e);case"ByteLevel":return new V(e);case"RobertaProcessing":return new J(e);case"BertProcessing":return new Z(e);case"Sequence":return new X(e);default:throw Error(`Unknown PostProcessor type: ${e.type}`)}}post_process(e,...t){throw Error("post_process should be implemented in subclass.")}_call(e,...t){return this.post_process(e,...t)}}class Z extends Y{constructor(e){super(e),this.cls=e.cls[0],this.sep=e.sep[0]}post_process(e,t=null,{add_special_tokens:s=!0}={}){s&&(e=(0,i.TR)([this.cls],e,[this.sep]));let n=Array(e.length).fill(0);if(null!==t){let r=s&&this instanceof J?[this.sep]:[],o=s?[this.sep]:[];e=(0,i.TR)(e,r,t,o),n=(0,i.TR)(n,Array(t.length+r.length+o.length).fill(1))}return{tokens:e,token_type_ids:n}}}class J extends Z{}class H extends Y{constructor(e){super(e),this.single=e.single,this.pair=e.pair}post_process(e,t=null,{add_special_tokens:s=!0}={}){let n=null===t?this.single:this.pair,r=[],o=[];for(let l of n)"SpecialToken"in l?s&&(r.push(l.SpecialToken.id),o.push(l.SpecialToken.type_id)):"Sequence"in l&&("A"===l.Sequence.id?(r=(0,i.TR)(r,e),o=(0,i.TR)(o,Array(e.length).fill(l.Sequence.type_id))):"B"===l.Sequence.id&&(r=(0,i.TR)(r,t),o=(0,i.TR)(o,Array(t.length).fill(l.Sequence.type_id))));return{tokens:r,token_type_ids:o}}}class V extends Y{post_process(e,t=null){return t&&(e=(0,i.TR)(e,t)),{tokens:e}}}class X extends Y{constructor(e){super(e),this.processors=e.processors.map(e=>Y.fromConfig(e))}post_process(e,t=null,s={}){let n;for(let i of this.processors)if(i instanceof V)e=i.post_process(e).tokens,t&&(t=i.post_process(t).tokens);else{let r=i.post_process(e,t,s);e=r.tokens,n=r.token_type_ids}return{tokens:e,token_type_ids:n}}}class Q extends n.b{constructor(e){super(),this.config=e,this.added_tokens=[],this.end_of_word_suffix=null,this.trim_offsets=e.trim_offsets}static fromConfig(e){if(null===e)return null;switch(e.type){case"WordPiece":return new ei(e);case"Metaspace":return new eu(e);case"ByteLevel":return new er(e);case"Replace":return new ee(e);case"ByteFallback":return new et(e);case"Fuse":return new es(e);case"Strip":return new en(e);case"Sequence":return new el(e);case"CTC":return new eo(e);case"BPEDecoder":return new ea(e);default:throw Error(`Unknown Decoder type: ${e.type}`)}}_call(e){return this.decode(e)}decode(e){return this.decode_chain(e).join("")}decode_chain(e){throw Error("`decode_chain` should be implemented in subclass.")}}class ee extends Q{decode_chain(e){let t=d(this.config.pattern);return null===t?e:e.map(e=>e.replaceAll(t,this.config.content))}}class et extends Q{constructor(e){super(e),this.text_decoder=new TextDecoder}decode_chain(e){let t=[],s=[];for(let n of e){let e=null;if(6===n.length&&n.startsWith("<0x")&&n.endsWith(">")){let t=parseInt(n.slice(3,5),16);isNaN(t)||(e=t)}if(null!==e)s.push(e);else{if(s.length>0){let e=this.text_decoder.decode(Uint8Array.from(s));t.push(e),s=[]}t.push(n)}}if(s.length>0){let e=this.text_decoder.decode(Uint8Array.from(s));t.push(e),s=[]}return t}}class es extends Q{decode_chain(e){return[e.join("")]}}class en extends Q{constructor(e){super(e),this.content=this.config.content,this.start=this.config.start,this.stop=this.config.stop}decode_chain(e){return e.map(e=>{let t=0;for(let s=0;s<this.start;++s){if(e[s]===this.content){t=s+1;continue}break}let s=e.length;for(let t=0;t<this.stop;++t){let n=e.length-t-1;if(e[n]===this.content){s=n;continue}break}return e.slice(t,s)})}}class ei extends Q{constructor(e){super(e),this.cleanup=e.cleanup}decode_chain(e){return e.map((e,t)=>(0!==t&&(e=e.startsWith(this.config.prefix)?e.replace(this.config.prefix,""):" "+e),this.cleanup&&(e=f(e)),e))}}class er extends Q{constructor(e){super(e),this.byte_decoder=E,this.text_decoder=new TextDecoder("utf-8",{fatal:!1,ignoreBOM:!0}),this.end_of_word_suffix=null}convert_tokens_to_string(e){let t=new Uint8Array([...e.join("")].map(e=>this.byte_decoder[e]));return this.text_decoder.decode(t)}decode_chain(e){let t=[],s=[];for(let n of e)void 0!==this.added_tokens.find(e=>e.content===n)?(s.length>0&&(t.push(this.convert_tokens_to_string(s)),s=[]),t.push(n)):s.push(n);return s.length>0&&t.push(this.convert_tokens_to_string(s)),t}}class eo extends Q{constructor(e){super(e),this.pad_token=this.config.pad_token,this.word_delimiter_token=this.config.word_delimiter_token,this.cleanup=this.config.cleanup}convert_tokens_to_string(e){if(0===e.length)return"";let t=[e[0]];for(let s=1;s<e.length;++s)e[s]!==t.at(-1)&&t.push(e[s]);let s=t.filter(e=>e!==this.pad_token).join("");return this.cleanup&&(s=f(s).replaceAll(this.word_delimiter_token," ").trim()),s}decode_chain(e){return[this.convert_tokens_to_string(e)]}}class el extends Q{constructor(e){super(e),this.decoders=e.decoders.map(e=>Q.fromConfig(e))}decode_chain(e){return this.decoders.reduce((e,t)=>t.decode_chain(e),e)}}class ea extends Q{constructor(e){super(e),this.suffix=this.config.suffix}decode_chain(e){return e.map((t,s)=>t.replaceAll(this.suffix,s===e.length-1?"":" "))}}class ec extends Q{decode_chain(e){let t="";for(let s=1;s<e.length;s+=2)t+=e[s];return[t]}}class eh extends q{constructor(e){super(),this.addPrefixSpace=e.add_prefix_space,this.replacement=e.replacement,this.strRep=e.str_rep||this.replacement,this.prepend_scheme=e.prepend_scheme??"always"}pre_tokenize_text(e,{section_index:t}={}){let s=e.replaceAll(" ",this.strRep);return this.addPrefixSpace&&!s.startsWith(this.replacement)&&("always"===this.prepend_scheme||"first"===this.prepend_scheme&&0===t)&&(s=this.strRep+s),[s]}}class eu extends Q{constructor(e){super(e),this.addPrefixSpace=e.add_prefix_space,this.replacement=e.replacement}decode_chain(e){let t=[];for(let s=0;s<e.length;++s){let n=e[s].replaceAll(this.replacement," ");this.addPrefixSpace&&0==s&&n.startsWith(" ")&&(n=n.substring(1)),t.push(n)}return t}}class ed extends C{constructor(e){super(e),this.charsmap=e.precompiled_charsmap}normalize(e){return e=(e=(e=e.replace(/[\u0001-\u0008\u000B\u000E-\u001F\u007F\u008F\u009F]/gm,"")).replace(/[\u0009\u000A\u000C\u000D\u00A0\u1680\u2000-\u200F\u2028\u2029\u202F\u205F\u2581\u3000\uFEFF\uFFFD]/gm," ")).includes("～")?e.split("～").map(e=>e.normalize("NFKC")).join("～"):e.normalize("NFKC")}}class e_ extends q{constructor(e){super(),this.tokenizers=e.pretokenizers.map(e=>q.fromConfig(e))}pre_tokenize_text(e,t){return this.tokenizers.reduce((e,s)=>s.pre_tokenize(e,t),[e])}}class ep extends q{constructor(e){super()}pre_tokenize_text(e,t){return e.match(/\w+|[^\w\s]+/g)||[]}}class ef extends q{constructor(e){super()}pre_tokenize_text(e,t){return e.match(/\S+/g)||[]}}class eg extends q{constructor(e){super(),this.config=e,this.pattern=d(this.config.pattern),this.content=this.config.content}pre_tokenize_text(e,t){return null===this.pattern?[e]:[e.replaceAll(this.pattern,this.config.content)]}}let ek=["bos_token","eos_token","unk_token","sep_token","pad_token","cls_token","mask_token"];class em extends n.b{return_token_type_ids=!1;padding_side="right";constructor(e,t){for(let s of(super(),this._tokenizer_config=t,this.normalizer=C.fromConfig(e.normalizer),this.pre_tokenizer=q.fromConfig(e.pre_tokenizer),this.model=v.fromConfig(e.model,t),this.post_processor=Y.fromConfig(e.post_processor),this.decoder=Q.fromConfig(e.decoder),this.special_tokens=[],this.all_special_ids=[],this.added_tokens=[],e.added_tokens)){let e=new y(s);this.added_tokens.push(e),this.model.tokens_to_ids.set(e.content,e.id),this.model.vocab[e.id]=e.content,e.special&&(this.special_tokens.push(e.content),this.all_special_ids.push(e.id))}if(this.additional_special_tokens=t.additional_special_tokens??[],this.special_tokens.push(...this.additional_special_tokens),this.special_tokens=[...new Set(this.special_tokens)],this.decoder&&(this.decoder.added_tokens=this.added_tokens,this.decoder.end_of_word_suffix=this.model.end_of_word_suffix),this.added_tokens_regex=this.added_tokens.length>0?new RegExp(this.added_tokens.slice().sort((e,t)=>t.content.length-e.content.length).map(e=>`${e.lstrip?"\\s*":""}(${(0,i.Nt)(e.content)})${e.rstrip?"\\s*":""}`).join("|")):null,this.mask_token=this.getToken("mask_token"),this.mask_token_id=this.model.tokens_to_ids.get(this.mask_token),this.pad_token=this.getToken("pad_token","eos_token"),this.pad_token_id=this.model.tokens_to_ids.get(this.pad_token),this.sep_token=this.getToken("sep_token"),this.sep_token_id=this.model.tokens_to_ids.get(this.sep_token),this.unk_token=this.getToken("unk_token"),this.unk_token_id=this.model.tokens_to_ids.get(this.unk_token),this.bos_token=this.getToken("bos_token"),this.bos_token_id=this.model.tokens_to_ids.get(this.bos_token),this.eos_token=this.getToken("eos_token"),this.eos_token_id=this.model.tokens_to_ids.get(this.eos_token),this.model_max_length=t.model_max_length,this.remove_space=t.remove_space,this.clean_up_tokenization_spaces=t.clean_up_tokenization_spaces??!0,this.do_lowercase_and_remove_accent=t.do_lowercase_and_remove_accent??!1,t.padding_side&&(this.padding_side=t.padding_side),this.legacy=!1,this.chat_template=t.chat_template??null,Array.isArray(this.chat_template)){let e=Object.create(null);for(let{name:t,template:s}of this.chat_template){if("string"!=typeof t||"string"!=typeof s)throw Error('Chat template must be a list of objects with "name" and "template" properties');e[t]=s}this.chat_template=e}this._compiled_template_cache=new Map}getToken(...e){for(let t of e){let e=this._tokenizer_config[t];if(e){if("object"!=typeof e)return e;if("AddedToken"===e.__type)return e.content;throw Error(`Unknown token: ${e}`)}}return null}static async from_pretrained(e,{progress_callback:t=null,config:s=null,cache_dir:n=null,local_files_only:i=!1,revision:r="main",legacy:o=null}={}){return new this(...await u(e,{progress_callback:t,config:s,cache_dir:n,local_files_only:i,revision:r,legacy:o}))}_call(e,{text_pair:t=null,add_special_tokens:s=!0,padding:n=!1,truncation:r=null,max_length:a=null,return_tensor:c=!0,return_token_type_ids:h=null}={}){let u;let d=Array.isArray(e);if(d){if(0===e.length)throw Error("text array must be non-empty");if(null!==t){if(Array.isArray(t)){if(e.length!==t.length)throw Error("text and text_pair must have the same length")}else throw Error("text_pair must also be an array");u=e.map((e,n)=>this._encode_plus(e,{text_pair:t[n],add_special_tokens:s,return_token_type_ids:h}))}else u=e.map(e=>this._encode_plus(e,{add_special_tokens:s,return_token_type_ids:h}))}else{if(null==e)throw Error("text may not be null or undefined");if(Array.isArray(t))throw Error("When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).");u=[this._encode_plus(e,{text_pair:t,add_special_tokens:s,return_token_type_ids:h})]}if(null===a?a="max_length"===n?this.model_max_length:(0,o.T9)(u.map(e=>e.input_ids.length))[0]:r||console.warn("Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=true` to explicitly truncate examples to max length."),a=Math.min(a,this.model_max_length??1/0),n||r)for(let e=0;e<u.length;++e)u[e].input_ids.length!==a&&(u[e].input_ids.length>a?r&&function(e,t){for(let s of Object.keys(e))e[s].length=t}(u[e],a):n&&function(e,t,s,n){for(let r of Object.keys(e)){let o=t-e[r].length,l=s(r),a=Array(o).fill(l);e[r]="right"===n?(0,i.TR)(e[r],a):(0,i.TR)(a,e[r])}}(u[e],a,e=>"input_ids"===e?this.pad_token_id:0,this.padding_side));let _={};if(c){if(!(n&&r)&&u.some(e=>{for(let t of Object.keys(e))if(e[t].length!==u[0][t]?.length)return!0;return!1}))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=true' and 'truncation=true' to have batched tensors with the same length.");let e=[u.length,u[0].input_ids.length];for(let t of Object.keys(u[0]))_[t]=new l.qY("int64",BigInt64Array.from(u.flatMap(e=>e[t]).map(BigInt)),e)}else{for(let e of Object.keys(u[0]))_[e]=u.map(t=>t[e]);if(!d)for(let e of Object.keys(_))_[e]=_[e][0]}return _}_encode_text(e){return null===e?null:(this.added_tokens_regex?e.split(this.added_tokens_regex).filter(e=>e):[e]).map((e,t)=>{if(void 0!==this.added_tokens.find(t=>t.content===e))return e;{if(!0===this.remove_space&&(e=e.trim().split(/\s+/).join(" ")),this.do_lowercase_and_remove_accent&&(e=g(e.toLowerCase())),null!==this.normalizer&&(e=this.normalizer(e)),0===e.length)return[];let s=null!==this.pre_tokenizer?this.pre_tokenizer(e,{section_index:t}):[e];return this.model(s)}}).flat()}_encode_plus(e,{text_pair:t=null,add_special_tokens:s=!0,return_token_type_ids:n=null}={}){let{tokens:i,token_type_ids:r}=this._tokenize_helper(e,{pair:t,add_special_tokens:s}),o=this.model.convert_tokens_to_ids(i),l={input_ids:o,attention_mask:Array(o.length).fill(1)};return(n??this.return_token_type_ids)&&r&&(l.token_type_ids=r),l}_tokenize_helper(e,{pair:t=null,add_special_tokens:s=!1}={}){let n=this._encode_text(e),r=this._encode_text(t);return this.post_processor?this.post_processor(n,r,{add_special_tokens:s}):{tokens:(0,i.TR)(n??[],r??[])}}tokenize(e,{pair:t=null,add_special_tokens:s=!1}={}){return this._tokenize_helper(e,{pair:t,add_special_tokens:s}).tokens}encode(e,{text_pair:t=null,add_special_tokens:s=!0,return_token_type_ids:n=null}={}){return this._encode_plus(e,{text_pair:t,add_special_tokens:s,return_token_type_ids:n}).input_ids}batch_decode(e,t={}){return e instanceof l.qY&&(e=e.tolist()),e.map(e=>this.decode(e,t))}decode(e,t={}){if(e instanceof l.qY&&(e=p(e)),!Array.isArray(e)||0===e.length||!(0,i.k1)(e[0]))throw Error("token_ids must be a non-empty array of integers.");return this.decode_single(e,t)}decode_single(e,{skip_special_tokens:t=!1,clean_up_tokenization_spaces:s=null}){let n=this.model.convert_ids_to_tokens(e);t&&(n=n.filter(e=>!this.special_tokens.includes(e)));let i=this.decoder?this.decoder(n):n.join(" ");return this.decoder&&this.decoder.end_of_word_suffix&&(i=i.replaceAll(this.decoder.end_of_word_suffix," "),t&&(i=i.trim())),(s??this.clean_up_tokenization_spaces)&&(i=f(i)),i}get_chat_template({chat_template:e=null,tools:t=null}={}){if(this.chat_template&&"object"==typeof this.chat_template){let s=this.chat_template;if(null!==e&&Object.hasOwn(s,e))e=s[e];else if(null===e){if(null!==t&&"tool_use"in s)e=s.tool_use;else if("default"in s)e=s.default;else throw Error(`This model has multiple chat templates with no default specified! Please either pass a chat template or the name of the template you wish to use to the 'chat_template' argument. Available template names are ${Object.keys(s).sort()}.`)}}else if(null===e){if(this.chat_template)e=this.chat_template;else throw Error("Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating")}return e}apply_chat_template(e,{tools:t=null,documents:s=null,chat_template:n=null,add_generation_prompt:i=!1,tokenize:r=!0,padding:o=!1,truncation:l=!1,max_length:a=null,return_tensor:h=!0,return_dict:u=!1,tokenizer_kwargs:d={},..._}={}){if("string"!=typeof(n=this.get_chat_template({chat_template:n,tools:t})))throw Error(`chat_template must be a string, but got ${typeof n}`);let p=this._compiled_template_cache.get(n);void 0===p&&(p=new c.Bj(n),this._compiled_template_cache.set(n,p));let f=Object.create(null);for(let e of ek){let t=this.getToken(e);t&&(f[e]=t)}let g=p.render({messages:e,add_generation_prompt:i,tools:t,documents:s,...f,..._});if(r){let e=this._call(g,{add_special_tokens:!1,padding:o,truncation:l,max_length:a,return_tensor:h,...d});return u?e:e.input_ids}return g}}class ex extends em{return_token_type_ids=!0}class ew extends em{return_token_type_ids=!0}class eb extends em{return_token_type_ids=!0}class ey extends em{return_token_type_ids=!0}class ev extends em{return_token_type_ids=!0}class ez extends em{return_token_type_ids=!0}class eA extends em{return_token_type_ids=!0}class eT extends em{return_token_type_ids=!0}class eE extends em{return_token_type_ids=!0}class eS extends em{}class eR extends em{}class eC extends em{return_token_type_ids=!0;constructor(e,t){super(e,t),console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}}class e$ extends em{return_token_type_ids=!0}class ej extends em{}class eN extends em{}class eF extends em{}class eM extends em{constructor(e,t){super(e,t),this.languageRegex=/^[a-z]{2}_[A-Z]{2}$/,this.language_codes=this.special_tokens.filter(e=>this.languageRegex.test(e)),this.lang_to_token=e=>e}_build_translation_inputs(e,t,s){return eJ(this,e,t,s)}}class eP extends eM{}class eO extends em{}class eW extends em{}class eL extends em{padding_side="left";constructor(e,t){super(e,t),this.legacy=t.legacy??!0,this.legacy||(this.normalizer=null,this.pre_tokenizer=new eh({replacement:"▁",add_prefix_space:!0,prepend_scheme:"first"}))}_encode_text(e){if(null===e)return null;if(this.legacy||0===e.length)return super._encode_text(e);let t=super._encode_text("▁"+e.replaceAll("▁"," "));return t.length>1&&"▁"===t[0]&&this.special_tokens.includes(t[1])&&(t=t.slice(1)),t}}class eU extends em{}class eq extends em{}class eB extends em{}class eI extends em{}class eD extends em{}class eK extends em{}class eG extends em{}class eY extends em{}class eZ extends em{}function eJ(e,t,s,n){if(!("language_codes"in e)||!Array.isArray(e.language_codes))throw Error("Tokenizer must have `language_codes` attribute set and it should be an array of language ids.");if(!("languageRegex"in e)||!(e.languageRegex instanceof RegExp))throw Error("Tokenizer must have `languageRegex` attribute set and it should be a regular expression.");if(!("lang_to_token"in e)||"function"!=typeof e.lang_to_token)throw Error("Tokenizer must have `lang_to_token` attribute set and it should be a function.");let i=n.src_lang,r=n.tgt_lang;if(!e.language_codes.includes(r))throw Error(`Target language code "${r}" is not valid. Must be one of: {${e.language_codes.join(", ")}}`);if(void 0!==i){if(!e.language_codes.includes(i))throw Error(`Source language code "${i}" is not valid. Must be one of: {${e.language_codes.join(", ")}}`);for(let t of e.post_processor.config.single)if("SpecialToken"in t&&e.languageRegex.test(t.SpecialToken.id)){t.SpecialToken.id=e.lang_to_token(i);break}}return n.forced_bos_token_id=e.model.convert_tokens_to_ids([e.lang_to_token(r)])[0],e._call(t,s)}class eH extends em{constructor(e,t){super(e,t),this.languageRegex=/^[a-z]{3}_[A-Z][a-z]{3}$/,this.language_codes=this.special_tokens.filter(e=>this.languageRegex.test(e)),this.lang_to_token=e=>e}_build_translation_inputs(e,t,s){return eJ(this,e,t,s)}}class eV extends em{constructor(e,t){super(e,t),this.languageRegex=/^__[a-z]{2,3}__$/,this.language_codes=this.special_tokens.filter(e=>this.languageRegex.test(e)).map(e=>e.slice(2,-2)),this.lang_to_token=e=>`__${e}__`}_build_translation_inputs(e,t,s){return eJ(this,e,t,s)}}class eX extends em{get timestamp_begin(){return this.model.convert_tokens_to_ids(["<|notimestamps|>"])[0]+1}_decode_asr(e,{return_timestamps:t=!1,return_language:s=!1,time_precision:n=null,force_full_sequences:i=!0}={}){if(null===n)throw Error("Must specify time_precision");let r=null,l="word"===t;function a(){return{language:r,timestamp:[null,null],text:""}}let c=[],u=a(),d=0,_=this.timestamp_begin,p=_+1500,f=[],g=[],k=!1,m=null,w=new Set(this.all_special_ids);for(let s of e){let e=s.tokens,i=l?s.token_timestamps:null,b=null,y=_;if("stride"in s){let[t,i,r]=s.stride;if(d-=i,m=t-r,i&&(y=i/n+_),r)for(let t=e.length-1;t>=0;--t){let s=Number(e[t]);if(s>=_){if(null!==b&&(s-_)*n<m)break;b=s}}}let v=[],z=[];for(let s=0;s<e.length;++s){let m=Number(e[s]);if(w.has(m)){let e=this.decode([m]),s=h.R8.get(e.slice(2,-2));if(void 0!==s){if(null!==r&&s!==r&&!t){f.push(v);let e=this.findLongestCommonSequence(f)[0],t=this.decode(e);u.text=t,c.push(u),f=[],v=[],u=a()}r=u.language=s}}else if(m>=_&&m<=p){let e=(m-_)*n+d,t=(0,o.LI)(e,2);if(null!==b&&m>=b)k=!0;else if(k||f.length>0&&m<y)k=!1;else if(null===u.timestamp[0])u.timestamp[0]=t;else if(t===u.timestamp[0]);else{u.timestamp[1]=t,f.push(v),l&&g.push(z);let[e,s]=this.findLongestCommonSequence(f,g),n=this.decode(e);u.text=n,l&&(u.words=this.collateWordTimestamps(e,s,r)),c.push(u),f=[],v=[],g=[],z=[],u=a()}}else if(v.push(m),l){let e,t=(0,o.LI)(i[s]+d,2);if(s+1<i.length){e=(0,o.LI)(i[s+1]+d,2);let r=this.decode([m]);x.test(r)&&(e=(0,o.LI)(Math.min(t+n,e),2))}else e=null;z.push([t,e])}}if("stride"in s){let[e,t,n]=s.stride;d+=e-n}v.length>0?(f.push(v),l&&g.push(z)):f.every(e=>0===e.length)&&(u=a(),f=[],v=[],g=[],z=[])}if(f.length>0){if(i&&t)throw Error("Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.");let[e,s]=this.findLongestCommonSequence(f,g),n=this.decode(e);u.text=n,l&&(u.words=this.collateWordTimestamps(e,s,r)),c.push(u)}let b=Object.create(null),y=c.map(e=>e.text).join("");if(t||s){for(let e=0;e<c.length;++e){let n=c[e];t||delete n.timestamp,s||delete n.language}if(l){let e=[];for(let t of c)for(let s of t.words)e.push(s);b={chunks:e}}else b={chunks:c}}return[y,b]}findLongestCommonSequence(e,t=null){let s=e[0],n=s.length,i=[],r=Array.isArray(t)&&t.length>0,o=r?[]:null,l=r?t[0]:null;for(let a=1;a<e.length;++a){let c=e[a],h=0,u=[n,n,0,0],d=c.length;for(let e=1;e<n+d;++e){let i;let o=Math.max(0,n-e),_=Math.min(n,n+d-e),p=s.slice(o,_),f=Math.max(0,e-n),g=Math.min(d,e),k=c.slice(f,g);if(p.length!==k.length)throw Error("There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.");i=r?p.filter((e,s)=>e===k[s]&&l[o+s]<=t[a][f+s]).length:p.filter((e,t)=>e===k[t]).length;let m=e/1e4,x=i/e+m;i>1&&x>h&&(h=x,u=[o,_,f,g])}let[_,p,f,g]=u,k=Math.floor((p+_)/2),m=Math.floor((g+f)/2);i.push(...s.slice(0,k)),n=(s=c.slice(m)).length,r&&(o.push(...l.slice(0,k)),l=t[a].slice(m))}return(i.push(...s),r)?(o.push(...l),[i,o]):[i,[]]}collateWordTimestamps(e,t,s){let[n,i,r]=this.combineTokensIntoWords(e,s),o=[];for(let e=0;e<n.length;++e){let s=r[e];o.push({text:n[e],timestamp:[t[s.at(0)][0],t[s.at(-1)][1]]})}return o}combineTokensIntoWords(e,t,s="\"'“\xa1\xbf([{-",n="\"'.。,，!！?？:：”)]}、"){let i,r,o;return["chinese","japanese","thai","lao","myanmar"].includes(t=t??"english")?[i,r,o]=this.splitTokensOnUnicode(e):[i,r,o]=this.splitTokensOnSpaces(e),this.mergePunctuations(i,r,o,s,n)}decode(e,t){let s;return t?.decode_with_timestamps?(e instanceof l.qY&&(e=p(e)),s=this.decodeWithTimestamps(e,t)):s=super.decode(e,t),s}decodeWithTimestamps(e,t){let s=t?.time_precision??.02,n=Array.from(this.all_special_ids).at(-1)+1,i=[[]];for(let t of e)if((t=Number(t))>=n){let e=((t-n)*s).toFixed(2);i.push(`<|${e}|>`),i.push([])}else i[i.length-1].push(t);return(i=i.map(e=>"string"==typeof e?e:super.decode(e,t))).join("")}splitTokensOnUnicode(e){let t=this.decode(e,{decode_with_timestamps:!0}),s=[],n=[],i=[],r=[],o=[],l=0;for(let a=0;a<e.length;++a){let c=e[a];r.push(c),o.push(a);let h=this.decode(r,{decode_with_timestamps:!0});h.includes("�")&&"�"!==t[l+h.indexOf("�")]||(s.push(h),n.push(r),i.push(o),r=[],o=[],l+=h.length)}return[s,n,i]}splitTokensOnSpaces(e){let[t,s,n]=this.splitTokensOnUnicode(e),i=[],r=[],o=[],l=RegExp(`^[${m}]$`,"gu");for(let e=0;e<t.length;++e){let a=t[e],c=s[e],h=n[e],u=c[0]>=this.model.tokens_to_ids.get("<|endoftext|>"),d=a.startsWith(" "),_=a.trim(),p=l.test(_);if(u||d||p||0===i.length)i.push(a),r.push(c),o.push(h);else{let e=i.length-1;i[e]+=a,r[e].push(...c),o[e].push(...h)}}return[i,r,o]}mergePunctuations(e,t,s,n,r){let o=structuredClone(e),l=structuredClone(t),a=structuredClone(s),c=o.length-2,h=o.length-1;for(;c>=0;)o[c].startsWith(" ")&&n.includes(o[c].trim())?(o[h]=o[c]+o[h],l[h]=(0,i.TR)(l[c],l[h]),a[h]=(0,i.TR)(a[c],a[h]),o[c]="",l[c]=[],a[c]=[]):h=c,--c;for(c=0,h=1;h<o.length;)!o[c].endsWith(" ")&&r.includes(o[h])?(o[c]+=o[h],l[c]=(0,i.TR)(l[c],l[h]),a[c]=(0,i.TR)(a[c],a[h]),o[h]="",l[h]=[],a[h]=[]):c=h,++h;return[o.filter(e=>e),l.filter(e=>e.length>0),a.filter(e=>e.length>0)]}}class eQ extends em{}class e0 extends em{}class e1 extends em{}class e2 extends em{constructor(e,t){super(e,t),this.languageRegex=/^(>>\w+<<)\s*/g,this.supported_language_codes=this.model.vocab.filter(e=>this.languageRegex.test(e)),console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}_encode_text(e){if(null===e)return null;let[t,...s]=e.trim().split(this.languageRegex);if(0===s.length)return super._encode_text(t);if(2===s.length){let[e,t]=s;return this.supported_language_codes.includes(e)||console.warn(`Unsupported language code "${e}" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`),(0,i.TR)([e],super._encode_text(t))}}}class e3 extends em{}class e8 extends em{}class e6 extends em{}class e9 extends em{}class e5 extends em{}class e7 extends em{constructor(e,t){super(e,t),this.decoder=new ec({})}}class e4 extends em{}class te extends em{}class tt{static TOKENIZER_CLASS_MAPPING={T5Tokenizer:ej,DistilBertTokenizer:eS,CamembertTokenizer:eR,DebertaTokenizer:ev,DebertaV2Tokenizer:ez,BertTokenizer:ex,HerbertTokenizer:eA,ConvBertTokenizer:eT,RoFormerTokenizer:eE,XLMTokenizer:eC,ElectraTokenizer:e$,MobileBertTokenizer:eb,SqueezeBertTokenizer:ey,AlbertTokenizer:ew,GPT2Tokenizer:eN,BartTokenizer:eF,MBartTokenizer:eM,MBart50Tokenizer:eP,RobertaTokenizer:eO,WhisperTokenizer:eX,CodeGenTokenizer:eQ,CLIPTokenizer:e0,SiglipTokenizer:e1,MarianTokenizer:e2,BloomTokenizer:eW,NllbTokenizer:eH,M2M100Tokenizer:eV,LlamaTokenizer:eL,CodeLlamaTokenizer:eU,XLMRobertaTokenizer:eq,MPNetTokenizer:eB,FalconTokenizer:eI,GPTNeoXTokenizer:eD,EsmTokenizer:eK,Wav2Vec2CTCTokenizer:e3,BlenderbotTokenizer:e8,BlenderbotSmallTokenizer:e6,SpeechT5Tokenizer:e9,NougatTokenizer:e5,VitsTokenizer:e7,Qwen2Tokenizer:eG,GemmaTokenizer:eY,Grok1Tokenizer:eZ,CohereTokenizer:e4,MgpstrTokenizer:te,PreTrainedTokenizer:em};static async from_pretrained(e,{progress_callback:t=null,config:s=null,cache_dir:n=null,local_files_only:i=!1,revision:r="main",legacy:o=null}={}){let[l,a]=await u(e,{progress_callback:t,config:s,cache_dir:n,local_files_only:i,revision:r,legacy:o}),c=a.tokenizer_class?.replace(/Fast$/,"")??"PreTrainedTokenizer",h=this.TOKENIZER_CLASS_MAPPING[c];return h||(console.warn(`Unknown tokenizer class "${c}", attempting to construct from base class.`),h=em),new h(l,a)}}}}]);